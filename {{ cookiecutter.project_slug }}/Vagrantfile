# -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.require_version ">= 2.2.5"

if Vagrant::Util::Platform.windows? then
  def running_as_admin?
    # query the LOCAL SERVICE account reg key (requires admin privileges)
    system('reg query "HKU\S-1-5-19"', :out => File::NULL)
  end

  unless running_as_admin?
    puts "Admin rights are required to create symlinks. Try running this Vagrantfile again from an admin command prompt."
    exit 1
  end
end

Vagrant.configure("2") do |config|

  config.vagrant.plugins = [
    "vagrant-proxyconf",
    "vagrant-vbguest",
    "vagrant-hostmanager",
  ]
  config.vm.box = "centos/8"
  config.vm.box_check_update = true
  # disable the default vagrant mount. We configure custom mounts on the VM separately.
  config.vm.synced_folder ".", "/vagrant", disabled: true

  config.yum_proxy.http = "{{ cookiecutter.vagrant_yum_proxy }}"
  config.proxy.http = "{{ cookiecutter.vagrant_http_proxy }}"
  config.proxy.https = "{{ cookiecutter.vagrant_https_proxy }}"

  config.vbguest.auto_update = true
  config.hostmanager.enabled = true
  config.hostmanager.manage_host = true
  config.hostmanager.manage_guest = true

  $dnf_proxy_script = <<-SCRIPT
    # the vagrant-proxyconf plugin doesn't support dnf yet, so we append
    # a proxy entry into the dnf.conf file here if applicable.
    VAGRANT_YUM_PROXY="{{ cookiecutter.vagrant_yum_proxy }}"
    if [ ! -z "$VAGRANT_YUM_PROXY" ]; then
      sed -n -i -e '/^proxy=/!p' -e "\\$aproxy=$VAGRANT_YUM_PROXY" /etc/dnf/dnf.conf
    fi
  SCRIPT

  config.vm.provision "shell", inline: $dnf_proxy_script, run: "always", privileged: true

  config.vm.define "dev", primary: true do |machine|
    machine.vm.hostname = "{{ cookiecutter.frontend_app_dev_hostname }}"
    machine.hostmanager.aliases = ["{{ cookiecutter.backend_app_dev_hostname }}"]
    machine.vm.network "private_network", ip: "172.17.177.21"
    # Enables X11 forwarding. This can be useful when running Selenium tests
    # since you can see exactly what's happening in the browser during the tests.
    # This requires an X11 server on your host machine, see https://www.xquartz.org
    # for Mac hosts or http://x.cygwin.com for Windows hosts.
    machine.ssh.forward_x11 = true
    machine.vm.synced_folder(
      "./backend_app",
      "/opt/{{ cookiecutter.project_slug }}_backend/current",
      type: "virtualbox",
      mount_options: ["uid=1234", "gid=1234"]
    )

    $dev_vm_post_up_script = <<-SCRIPT
      # Ensure that the 'venv' folder is mounted after vagrant has configured
      # its synced folders. Restart the backend app service - systemd may set
      # this unit as failed shortly after boot since the files within the
      # vagrant mount won't be available initially. These commands would fail on
      # an initial 'vagrant up' without '|| true' since this script is executed
      # *before* the first ansible provisioning is run (which adds this mount
      # configuration into /etc/fstab and creates the systemd service).
      mount /opt/{{ cookiecutter.project_slug }}_backend/current/venv &> /dev/null || true
      systemctl restart {{ cookiecutter.project_slug }} &> /dev/null || true
    SCRIPT

    machine.vm.provision "shell", inline: $dev_vm_post_up_script, run: "always", privileged: true

    machine.vm.provider "virtualbox" do |virtualbox|
      virtualbox.name = "{{ cookiecutter.project_slug }}_dev"
      virtualbox.memory = 1024
    end
  end

  config.vm.define "db" do |machine|
    machine.vbguest.no_install = true
    machine.vm.hostname = "{{ cookiecutter.database_dev_hostname }}"
    machine.vm.network "private_network", ip: "172.17.177.22"
    machine.vm.provider "virtualbox" do |virtualbox|
      virtualbox.name = "{{ cookiecutter.project_slug }}_db"
      virtualbox.memory = 512
    end
  end

  # We spin up another VM and execute ansible using the ansible_local vagrant
  # provisioner, as opposed to executing ansible directly from the host machine.
  #
  # This helps simplify the initial setup process, creates a  consistent
  # provisioning environment and allows us to provision a  dev environment on
  # host machines where Ansible is not supported, such as Windows.
  # (see https://docs.ansible.com/ansible/latest/user_guide/windows_faq.html#can-ansible-run-on-windows )
  #
  # However, this process does result in the creation of a 'controller' VM,
  # which increases the system resources used for this dev environment.
  config.vm.define "controller" do |machine|
    # By default Vagrant adds the host's name to the loopback address when
    # setting machine.vm.hostname. This can result in unexpected behaviour
    # when attempting to bind to the host, like the frontend app dev server does
    # when running 'yarn start'. So we instead use a hostname alias and set the
    # hostname later on ourselves in an inline script.
    # See https://github.com/hashicorp/vagrant/issues/7263 for more.
    machine.hostmanager.aliases = ["{{ cookiecutter.controller_hostname }}"]
    machine.vm.network "private_network", ip: "172.17.177.20"
    machine.vm.synced_folder "./", "/vagrant", type: "virtualbox"
    # Mount the folder containing the dev vm ssh keys with explicit file
    # permissions. This addresses 'unprotected private key file' errors on
    # Windows, due to mounts on Windows having 777 file permissions by default.
    machine.vm.synced_folder(
      "./.vagrant/machines",
      "/vagrant_machines",
      type: "virtualbox",
      mount_options: ["fmode=600"]
    )

    machine.vm.provider "virtualbox" do |virtualbox|
      virtualbox.name = "{{ cookiecutter.project_slug }}_controller"
      virtualbox.memory = 512
    end

    $controller_vm_setup_script = <<-SCRIPT
      hostnamectl set-hostname {{ cookiecutter.controller_hostname }}
      # install ansible requirements
      dnf install -y epel-release
      dnf install -y python3 python3-pip sshpass openssl-devel
      pip3 install -r /vagrant/provisioning/requirements.txt
    SCRIPT

    machine.vm.provision "shell", inline: $controller_vm_setup_script, privileged: true

    # See https://www.vagrantup.com/docs/provisioning/ansible_local.html and
    # https://www.vagrantup.com/docs/provisioning/ansible_common.html
    machine.vm.provision "ansible_local" do |ansible|
      ansible.compatibility_mode = "2.0"
      ansible.playbook = "playbook.yml"
      ansible.provisioning_path = "/vagrant/provisioning"
      ansible.config_file = "ansible-vagrant.cfg"
      ansible.install = false
      ansible.galaxy_role_file = "requirements.yml"
      ansible.galaxy_command = "ansible-galaxy install --role-file=%{role_file}"
      ansible.verbose = false
      ansible.limit = "all,localhost"
      ansible.inventory_path = "environments/dev/inventory"
    end
    # Shut down the controller VM once it has finished provisioning the other VMs.
    # machine.vm.provision "shell", inline: "shutdown -h now"
  end

end
